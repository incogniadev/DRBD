# Creaci√≥n de VMs desde shell de Proxmox

Esta gu√≠a describe c√≥mo crear las m√°quinas virtuales necesarias para el laboratorio DRBD directamente desde la l√≠nea de comandos de Proxmox.

## Requisitos previos

- **Acceso privilegiado**: SSH como `root` o usuario con permisos para gestionar VMs
- **ISO personalizada recomendada**: `debian/debian-12.11.0-amd64-preseed.iso` (instalaci√≥n automatizada)
- **ISO alternativa**: Template o ISO est√°ndar de Debian 12.11+ para instalaci√≥n manual
- **Red bridge `vmbr2` configurada**: Bridge de red dedicado para el laboratorio
- **Espacio suficiente en almacenamiento**: M√≠nimo 200GB disponibles

### üåê Configuraci√≥n de red (vmbr2)

**¬øPor qu√© vmbr2 y no vmbr0?**
- `vmbr0` suele estar configurado para la red de administraci√≥n de Proxmox
- `vmbr2` se usa como red dedicada para el laboratorio DRBD, aislando el tr√°fico del cl√∫ster
- Permite configurar una subred espec√≠fica (`192.168.10.0/24`) sin conflictos

**Configuraci√≥n requerida de vmbr2:**
```bash
# Ejemplo de configuraci√≥n en /etc/network/interfaces
auto vmbr2
iface vmbr2 inet static
    address 192.168.10.1/24
    bridge-ports none
    bridge-stp off
    bridge-fd 0
```

### ‚ö†Ô∏è Importante: Instalaci√≥n escalonada recomendada

**Aunque las VMs se pueden crear simult√°neamente con scripts, se recomienda encarecidamente ejecutar las instalaciones de forma escalonada** para evitar colisiones de paquetes durante la instalaci√≥n automatizada de Debian, ya que todas usan temporalmente la misma IP (10.0.0.69) durante el proceso de instalaci√≥n.

### üìù Nota sobre configuraci√≥n post-instalaci√≥n

**Importante**: Aunque la instalaci√≥n de Debian es completamente desatendida (no solicita ning√∫n par√°metro), **la configuraci√≥n de IP final y hostname debe realizarse manualmente** en cada VM despu√©s de que complete la instalaci√≥n. Cada nodo arranca inicialmente con IP temporal `10.0.0.69` y hostname gen√©rico.

## Especificaciones de las VMs

### Nodos DRBD (Node1 y Node2)
- **CPU**: 2 vCPUs
- **RAM**: 4GB 
- **Disco principal**: 24GB (sistema operativo)
- **Disco secundario**: 16GB (DRBD storage)
- **Red**: 2x vmbr2 (administraci√≥n + cl√∫ster)
- **OS**: Debian 12.11+

### Host Docker (Node3)
- **CPU**: 2 vCPUs
- **RAM**: 4GB
- **Disco**: 32GB (sistema + contenedores)
- **Red**: 2x vmbr2 (administraci√≥n + cl√∫ster)
- **OS**: Debian 12.11+

## Creaci√≥n de VMs desde shell de Proxmox

### 1. Conectarse a Proxmox host

```bash
ssh root@<proxmox-host-ip>
```

### 2. Crear VM Node1 (DRBD Primario)

```bash
# Crear VM con ID 231
qm create 231 --name "node1-drbd" --memory 4096 --cores 2 --sockets 1 --cpu host --ostype l26 --machine q35 --scsihw virtio-scsi-pci --bootdisk scsi0 --scsi0 local-lvm:24,format=raw --scsi1 local-lvm:16,format=raw --efidisk0 local-lvm:1,format=raw --net0 virtio,bridge=vmbr2 --net1 virtio,bridge=vmbr2 --agent 1 --bios ovmf --onboot 1

# Configurar orden de boot
qm set 231 --boot order=scsi0

# Adjuntar ISO de instalaci√≥n (ajustar path seg√∫n tu setup)
qm set 231 --cdrom local:iso/debian-12.11.0-amd64-preseed.iso
```

### 3. Crear VM Node2 (DRBD Secundario)

```bash
# Crear VM con ID 232
qm create 232 --name "node2-drbd" --memory 4096 --cores 2 --sockets 1 --cpu host --ostype l26 --machine q35 --scsihw virtio-scsi-pci --bootdisk scsi0 --scsi0 local-lvm:24,format=raw --scsi1 local-lvm:16,format=raw --efidisk0 local-lvm:1,format=raw --net0 virtio,bridge=vmbr2 --net1 virtio,bridge=vmbr2 --agent 1 --bios ovmf --onboot 1

# Configurar orden de boot
qm set 232 --boot order=scsi0

# Adjuntar ISO de instalaci√≥n
qm set 232 --cdrom local:iso/debian-12.11.0-amd64-preseed.iso
```

### 4. Crear VM Node3 (Docker Host)

```bash
# Crear VM con ID 233
qm create 233 --name "node3-docker" --memory 4096 --cores 2 --sockets 1 --cpu host --ostype l26 --machine q35 --scsihw virtio-scsi-pci --bootdisk scsi0 --scsi0 local-lvm:32,format=raw --efidisk0 local-lvm:1,format=raw --net0 virtio,bridge=vmbr2 --net1 virtio,bridge=vmbr2 --agent 1 --bios ovmf --onboot 1

# Configurar orden de boot
qm set 233 --boot order=scsi0

# Adjuntar ISO de instalaci√≥n
qm set 233 --cdrom local:iso/debian-12.11.0-amd64-preseed.iso
```

### 5. Configurar red adicional para cl√∫ster (opcional)

Si necesitas una segunda interfaz de red para separar tr√°fico de administraci√≥n del cl√∫ster:

```bash
# Nota: Las interfaces ya est√°n configuradas en el comando de creaci√≥n
# Si necesitas modificar las interfaces despu√©s de la creaci√≥n:
qm set 231 --net1 virtio,bridge=vmbr2
qm set 232 --net1 virtio,bridge=vmbr2
qm set 233 --net1 virtio,bridge=vmbr2
```

### 6. Iniciar las VMs (M√©todo escalonado recomendado)

‚ö†Ô∏è **Importante**: Para evitar colisiones de paquetes durante la instalaci√≥n automatizada, inicia las VMs de forma escalonada:

```bash
# M√©todo 1: Instalaci√≥n escalonada (RECOMENDADO)
# Iniciar Node1 primero
qm start 231
echo "Esperando instalaci√≥n de Node1... (aproximadamente 10 minutos)"
# Monitorear progreso: qm vncproxy 231

# Una vez que Node1 complete su instalaci√≥n y se reinicie, iniciar Node2
# qm start 232  # Ejecutar cuando Node1 est√© listo

# Finalmente, cuando Node2 complete, iniciar Node3
# qm start 233  # Ejecutar cuando Node2 est√© listo

# Verificar estado de todas las VMs
qm list | grep -E "(231|232|233)"
```

```bash
# M√©todo 2: Inicio simult√°neo (solo si usas instalaci√≥n manual)
# Solo usar este m√©todo si NO est√°s usando la ISO con preseed
qm start 231 && qm start 232 && qm start 233
qm list
```

## üöÄ Instalaci√≥n automatizada con ISO personalizada (Recomendado)

### Uso de debian-12.11.0-amd64-preseed.iso

Si est√°s usando la ISO personalizada, la instalaci√≥n ser√° completamente automatizada:

```bash
# 1. Las VMs arrancar√°n autom√°ticamente desde la ISO
# 2. Tras 5 segundos se seleccionar√° "Automated Install (Preseed)"
# 3. La instalaci√≥n proceder√° sin intervenci√≥n manual
# 4. Al finalizar, el sistema se reiniciar√° autom√°ticamente

# Para monitorear el progreso (opcional):
qm vncproxy 231  # Ver la instalaci√≥n en Node1
qm vncproxy 232  # Ver la instalaci√≥n en Node2
qm vncproxy 233  # Ver la instalaci√≥n en Node3
```

### Configuraci√≥n post-instalaci√≥n (Manual requerida)

‚ö†Ô∏è **Importante**: Aunque la instalaci√≥n de Debian es desatendida, **cada VM requiere configuraci√≥n manual individual** para establecer su IP final y hostname.

Durante la instalaci√≥n automatizada, el script `config-network.sh` se copia al directorio home del usuario `incognia` y debe ejecutarse manualmente en cada VM.

#### Proceso para cada VM:

```bash
# 1. Conectarse v√≠a SSH a cada VM (todas inician con IP temporal)
ssh incognia@10.0.0.69

# 2. Ejecutar el script de reconfiguraci√≥n (ya incluido durante la instalaci√≥n)
sudo ./config-network.sh
```

#### üìù Qu√© hace el script config-network.sh:
- üîç **Muestra configuraci√≥n actual** (IP, hostname, FQDN)
- ‚öôÔ∏è **Solicita interactivamente**:
  - Nueva IP con notaci√≥n CIDR (ej: `192.168.10.231/24`)
  - Gateway (calcula sugerencia autom√°ticamente)
  - Nuevo hostname (ej: `node1`)
  - Dominio (predeterminado: `faraday.org.mx`)
- üîÑ **Aplica cambios**:
  - Actualiza `/etc/hostname`, `/etc/hosts`, `/etc/network/interfaces`
  - Maneja conflictos con NetworkManager/systemd-networkd
  - Reinicia servicios de red autom√°ticamente
  - Verifica conectividad
- üíæ **Crea respaldos** de configuraciones anteriores
- ‚öôÔ∏è **Ofrece reinicio** del sistema para asegurar cambios

#### üéØ IPs finales objetivo:
- **Node1**: `192.168.10.231/24` (hostname: `node1`)
- **Node2**: `192.168.10.232/24` (hostname: `node2`) 
- **Node3**: `192.168.10.233/24` (hostname: `node3-docker`)

üîÑ **Repetir este proceso para cada una de las 3 VMs** antes de proceder con la configuraci√≥n de DRBD.

üìÅ **Nota**: El script es inteligente y maneja m√∫ltiples m√©todos de aplicaci√≥n de red, respaldos autom√°ticos y validaciones de entrada.

**‚ÑπÔ∏è Para m√°s detalles**: Ver [debian/README.md](../debian/README.md) para documentaci√≥n completa.

---

## üõ†Ô∏è Instalaci√≥n manual (M√©todo tradicional)

### 1. Acceder a las VMs e instalar Debian manualmente

```bash
# Acceder a consola de VM para instalaci√≥n manual
qm monitor 231
# Seguir proceso de instalaci√≥n de Debian tradicional

# O usar VNC si est√° disponible
qm vncproxy 231
```

### 2. Configuraci√≥n de red despu√©s de la instalaci√≥n

En cada VM, configurar la red seg√∫n el esquema de IPs usando el m√©todo tradicional de Debian con `/etc/network/interfaces`:

#### Node1 (192.168.10.231)
```bash
cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto ens18
iface ens18 inet static
    address 192.168.10.231
    netmask 255.255.255.0
    gateway 192.168.10.1
    dns-nameservers 8.8.8.8 8.8.4.4
EOF

# Reiniciar servicios de red
systemctl restart networking
```

#### Node2 (192.168.10.232)
```bash
cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto ens18
iface ens18 inet static
    address 192.168.10.232
    netmask 255.255.255.0
    gateway 192.168.10.1
    dns-nameservers 8.8.8.8 8.8.4.4
EOF

# Reiniciar servicios de red
systemctl restart networking
```

#### Node3 (192.168.10.233)
```bash
cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto ens18
iface ens18 inet static
    address 192.168.10.233
    netmask 255.255.255.0
    gateway 192.168.10.1
    dns-nameservers 8.8.8.8 8.8.4.4
EOF

# Reiniciar servicios de red
systemctl restart networking
```

### 3. Configurar hostnames y resoluci√≥n

En cada VM:

```bash
# Node1
hostnamectl set-hostname node1
echo "127.0.1.1 node1" >> /etc/hosts

# Node2  
hostnamectl set-hostname node2
echo "127.0.1.1 node2" >> /etc/hosts

# Node3
hostnamectl set-hostname node3-docker
echo "127.0.1.1 node3-docker" >> /etc/hosts

# En todas las VMs, agregar resoluci√≥n de nombres del cl√∫ster
cat >> /etc/hosts << EOF
192.168.10.231    node1
192.168.10.232    node2  
192.168.10.233    node3-docker
192.168.10.230    cluster-vip
EOF
```

## Script de automatizaci√≥n

### Crear todas las VMs con un script

```bash
#!/bin/bash
# create-drbd-vms.sh

# Variables
ISO_PATH="local:iso/debian-12.11.0-amd64-preseed.iso"
STORAGE="local-lvm"

echo "Creando VM Node1 (DRBD Primario)..."
qm create 231 \
  --name "node1-drbd" \
  --memory 4096 \
  --cores 2 \
  --sockets 1 \
  --cpu host \
  --ostype l26 \
  --machine q35 \
  --scsihw virtio-scsi-pci \
  --bootdisk scsi0 \
  --scsi0 ${STORAGE}:24,format=raw \
  --scsi1 ${STORAGE}:16,format=raw \
  --efidisk0 ${STORAGE}:1,format=raw \
  --net0 virtio,bridge=vmbr2 \
  --net1 virtio,bridge=vmbr2 \
  --agent 1 \
  --bios ovmf \
  --onboot 1 \
  --cdrom ${ISO_PATH} \
  --boot order=scsi0

echo "Creando VM Node2 (DRBD Secundario)..."
qm create 232 \
  --name "node2-drbd" \
  --memory 4096 \
  --cores 2 \
  --sockets 1 \
  --cpu host \
  --ostype l26 \
  --machine q35 \
  --scsihw virtio-scsi-pci \
  --bootdisk scsi0 \
  --scsi0 ${STORAGE}:24,format=raw \
  --scsi1 ${STORAGE}:16,format=raw \
  --efidisk0 ${STORAGE}:1,format=raw \
  --net0 virtio,bridge=vmbr2 \
  --net1 virtio,bridge=vmbr2 \
  --agent 1 \
  --bios ovmf \
  --onboot 1 \
  --cdrom ${ISO_PATH} \
  --boot order=scsi0

echo "Creando VM Node3 (Docker Host)..."
qm create 233 \
  --name "node3-docker" \
  --memory 4096 \
  --cores 2 \
  --sockets 1 \
  --cpu host \
  --ostype l26 \
  --machine q35 \
  --scsihw virtio-scsi-pci \
  --bootdisk scsi0 \
  --scsi0 ${STORAGE}:32,format=raw \
  --efidisk0 ${STORAGE}:1,format=raw \
  --net0 virtio,bridge=vmbr2 \
  --net1 virtio,bridge=vmbr2 \
  --agent 1 \
  --bios ovmf \
  --onboot 1 \
  --cdrom ${ISO_PATH} \
  --boot order=scsi0

echo "‚ö†Ô∏è  IMPORTANTE: Para evitar colisiones durante la instalaci√≥n automatizada,"
echo "se recomienda iniciar las VMs de forma escalonada:"
echo ""
echo "1. Iniciar Node1 primero:"
echo "   qm start 231"
echo "   # Esperar ~10 minutos para que complete la instalaci√≥n"
echo ""
echo "2. Cuando Node1 est√© listo, iniciar Node2:"
echo "   qm start 232"
echo "   # Esperar ~10 minutos para que complete la instalaci√≥n"
echo ""
echo "3. Finalmente, iniciar Node3:"
echo "   qm start 233"
echo ""
echo "Monitorear progreso con: qm vncproxy <vm-id>"
echo ""
echo "--- Solo para instalaci√≥n simult√°nea (NO recomendado con preseed) ---"
echo "Para iniciar todas las VMs ahora (usar solo con instalaci√≥n manual):"
echo "qm start 231 && qm start 232 && qm start 233"
echo ""
echo "Estado actual de las VMs:"
qm list | grep -E "(231|232|233)"
echo ""
echo "‚úÖ Configuraci√≥n de VMs completada. Proceder con instalaci√≥n escalonada."
```

## Comandos √∫tiles de Proxmox

### Gesti√≥n de VMs

```bash
# Listar todas las VMs
qm list

# Ver configuraci√≥n de una VM espec√≠fica (ejemplos con nuestras VMs)
qm config 231  # Node1 DRBD
qm config 232  # Node2 DRBD
qm config 233  # Node3 Docker

# Detener una VM
qm stop 231    # Detener Node1
qm stop 232    # Detener Node2
qm stop 233    # Detener Node3

# Reiniciar una VM
qm reboot 231  # Reiniciar Node1
qm reboot 232  # Reiniciar Node2
qm reboot 233  # Reiniciar Node3

# Eliminar una VM (¬°CUIDADO!)
qm destroy 231 # Eliminar Node1
qm destroy 232 # Eliminar Node2
qm destroy 233 # Eliminar Node3

# Crear snapshot antes de configurar DRBD
qm snapshot 231 pre-drbd-config
qm snapshot 232 pre-drbd-config
qm snapshot 233 pre-docker-config

# Restaurar snapshot si algo sale mal
qm rollback 231 pre-drbd-config
qm rollback 232 pre-drbd-config
qm rollback 233 pre-docker-config

# Clonar VM para respaldo
qm clone 231 234 --name node1-backup
qm clone 232 235 --name node2-backup
qm clone 233 236 --name node3-backup
```

### Monitoreo

```bash
# Ver uso de recursos de las VMs del laboratorio
qm monitor 231  # Monitorear Node1
qm monitor 232  # Monitorear Node2
qm monitor 233  # Monitorear Node3

# Ver logs de las VMs
qm log 231     # Logs de Node1
qm log 232     # Logs de Node2
qm log 233     # Logs de Node3

# Acceso VNC para instalaci√≥n/troubleshooting
qm vncproxy 231  # VNC a Node1
qm vncproxy 232  # VNC a Node2
qm vncproxy 233  # VNC a Node3

# Verificar estado de todas las VMs del laboratorio
qm list | grep -E "(231|232|233)"

# Iniciar todas las VMs del laboratorio (m√©todo escalonado recomendado)
qm start 231 && sleep 600 && qm start 232 && sleep 600 && qm start 233

# Detener todas las VMs del laboratorio en orden
qm stop 233 && qm stop 232 && qm stop 231
```

## Consideraciones importantes

1. **Snapshots**: Crear snapshots antes de configurar DRBD y Pacemaker
2. **Recursos**: Ajustar CPU y RAM seg√∫n recursos disponibles en Proxmox
3. **Storage**: Verificar que el storage backend tenga espacio suficiente
4. **Red**: Asegurarse de que vmbr2 est√© configurado correctamente
5. **ISO**: Verificar que la ISO de Debian est√© disponible en el path especificado

## Pr√≥ximos pasos

1. Instalar Debian en cada VM
2. Configurar SSH y acceso remoto
3. Seguir la [gu√≠a de instalaci√≥n DRBD](INSTALLATION.md)
4. Implementar la configuraci√≥n espec√≠fica de [Proxmox con Debian](PROXMOX_DEBIAN.md)

---

**Autor**: Rodrigo √Ålvarez (@incogniadev)  
**Fecha**: 2025-07-21
