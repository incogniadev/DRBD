# Configuraci√≥n post-instalaci√≥n para Debian en Proxmox

Esta gu√≠a cubre la configuraci√≥n espec√≠fica de software despu√©s de instalar Debian en las m√°quinas virtuales del laboratorio DRBD.

## üöÄ Instalaci√≥n automatizada (Recomendado)

Para una instalaci√≥n completamente desatendida, usa la **ISO personalizada con preseed**:

### 1. Usar la ISO personalizada
```bash
# En Proxmox, usar la ISO personalizada:
debian/debian-12.11.0-amd64-preseed.iso

# Esta ISO incluye:
# - Instalaci√≥n completamente automatizada
# - Usuario 'incognia' preconfigurado con SSH y sudo
# - Paquetes esenciales preinstalados
# - Configuraci√≥n de red inicial (10.0.0.69/8)
```

### 2. Proceso de instalaci√≥n automatizada
1. **Arranque autom√°tico**: Tras 5 segundos se selecciona "Automated Install (Preseed)"
2. **Instalaci√≥n desatendida**: Sin intervenci√≥n manual requerida
3. **Usuario preconfigurado**: 
   - Usuario: `incognia`
   - Grupos: `sudo`, `ssh-users`
   - Llave SSH instalada autom√°ticamente
4. **Paquetes preinstalados**: SSH, herramientas de sistema, mc, btop, neofetch
5. **Reinicio autom√°tico**: El sistema se reinicia y queda listo para usar

### 3. Reconfiguraci√≥n post-instalaci√≥n

Despu√©s de la instalaci√≥n automatizada, reconfigurar la red para cada nodo:

```bash
# Conectarse v√≠a SSH (la instalaci√≥n usa IP 10.0.0.69)
ssh incognia@10.0.0.69

# Ejecutar script de reconfiguraci√≥n de red
sudo ./config-network.sh

# Seguir las instrucciones para configurar:
# - Node1: 192.168.10.231/24
# - Node2: 192.168.10.232/24  
# - Node3: 192.168.10.233/24
```

**‚ÑπÔ∏è Informaci√≥n detallada**: Ver [debian/README.md](../debian/README.md) para documentaci√≥n completa de la instalaci√≥n automatizada.

---

## üõ†Ô∏è Instalaci√≥n manual (M√©todo tradicional)

### Prerrequisitos para instalaci√≥n manual

- VMs creadas seg√∫n [PROXMOX_VM_CREATION.md](PROXMOX_VM_CREATION.md)
- Debian 12.11+ instalado manualmente en todas las VMs
- Acceso SSH o consola a las m√°quinas virtuales
- Red configurada con acceso a internet para descarga de paquetes

## Configuraci√≥n inicial del sistema

### 1. Configuraci√≥n inicial como root (en todas las VMs)

**Nota**: Estos comandos deben ejecutarse como `root` inmediatamente despu√©s de la instalaci√≥n de Debian.

```bash
# Actualizar el sistema
apt update && apt upgrade -y

# Instalar sudo y utilidades b√°sicas
apt install -y sudo wget curl vim nano net-tools htop tree

# Agregar usuario incognia (creado durante instalaci√≥n) al grupo sudo
usermod -aG sudo incognia

# Verificar que el usuario est√° en el grupo sudo
groups incognia

# Instalar agente QEMU para mejor integraci√≥n con Proxmox
apt install -y qemu-guest-agent
systemctl enable qemu-guest-agent
systemctl start qemu-guest-agent

# Cambiar a usuario incognia para el resto de la configuraci√≥n
su - incognia
```

### 2. Verificaci√≥n de configuraci√≥n de usuario

```bash
# Como usuario incognia, verificar que sudo funciona
sudo whoami
# Deber√≠a devolver: root

# Verificar conectividad a internet
ping -c 3 8.8.8.8
```

### 2. Configuraci√≥n de red dual (todas las VMs)

**Nota**: Esta configuraci√≥n asume que las VMs tienen dos interfaces de red como se especifica en la gu√≠a de creaci√≥n. Debian Server usa el m√©todo tradicional con `/etc/network/interfaces`.

#### Node1 (192.168.10.231)
```bash
sudo cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# Primary network interface (administration)
auto ens18
iface ens18 inet static
    address 10.0.0.231
    netmask 255.0.0.0
    gateway 10.0.0.1
    dns-nameservers 8.8.8.8 8.8.4.4

# Secondary network interface (cluster)
auto ens19
iface ens19 inet static
    address 192.168.10.231
    netmask 255.255.255.0
EOF

# Reiniciar servicios de red
sudo systemctl restart networking
```

#### Node2 (192.168.10.232)
```bash
sudo cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# Primary network interface (administration)
auto ens18
iface ens18 inet static
    address 10.0.0.232
    netmask 255.0.0.0
    gateway 10.0.0.1
    dns-nameservers 8.8.8.8 8.8.4.4

# Secondary network interface (cluster)
auto ens19
iface ens19 inet static
    address 192.168.10.232
    netmask 255.255.255.0
EOF

# Reiniciar servicios de red
sudo systemctl restart networking
```

#### Node3 (192.168.10.233)
```bash
sudo cat > /etc/network/interfaces << EOF
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# Primary network interface (administration)
auto ens18
iface ens18 inet static
    address 10.0.0.233
    netmask 255.0.0.0
    gateway 10.0.0.1
    dns-nameservers 8.8.8.8 8.8.4.4

# Secondary network interface (cluster)
auto ens19
iface ens19 inet static
    address 192.168.10.233
    netmask 255.255.255.0
EOF

# Reiniciar servicios de red
sudo systemctl restart networking
```

### 3. Configurar hostnames y resoluci√≥n DNS

#### En Node1:
```bash
sudo hostnamectl set-hostname node1
echo "127.0.1.1 node1" | sudo tee -a /etc/hosts
```

#### En Node2:
```bash
sudo hostnamectl set-hostname node2
echo "127.0.1.1 node2" | sudo tee -a /etc/hosts
```

#### En Node3:
```bash
sudo hostnamectl set-hostname node3-docker
echo "127.0.1.1 node3-docker" | sudo tee -a /etc/hosts
```

#### En todas las VMs, agregar resoluci√≥n de nombres del cl√∫ster:
```bash
cat << EOF | sudo tee -a /etc/hosts
192.168.10.231    node1
192.168.10.232    node2  
192.168.10.233    node3-docker
192.168.10.230    cluster-vip
EOF
```

## Instalaci√≥n de software espec√≠fico

### 1. Instalaci√≥n de DRBD en nodos 1 y 2

```bash
# DRBD puede requerir el m√≥dulo del kernel
# Verificar si el m√≥dulo est√° disponible
modprobe drbd

# Si no est√° disponible, instalar headers del kernel
apt install -y linux-headers-$(uname -r)

# Instalar DRBD desde repositorios oficiales
apt install -y drbd-utils drbd-dkms

# Verificar instalaci√≥n
lsmod | grep drbd
drbdadm --version
```

### 2. Configuraci√≥n de Pacemaker en nodos 1 y 2

```bash
# Instalar pacemaker y herramientas
apt install -y pacemaker corosync pcs crmsh

# Habilitar servicios
systemctl enable pacemaker
systemctl enable corosync
systemctl enable pcsd

# Configurar usuario hacluster
passwd hacluster
# Usar la misma contrase√±a en ambos nodos

# Iniciar servicio pcsd
systemctl start pcsd
```

### 3. Instalaci√≥n de Docker en Node3

```bash
# Instalar Docker usando el script oficial
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Agregar usuario actual al grupo docker
sudo usermod -aG docker $USER

# Habilitar y iniciar Docker
sudo systemctl enable docker
sudo systemctl start docker

# Verificar instalaci√≥n
docker --version
sudo docker run hello-world
```

### 4. Configuraci√≥n de firewall en nodos DRBD

```bash
# Si ufw est√° habilitado, configurar reglas para el cl√∫ster (solo en nodos 1 y 2)
sudo ufw allow 7789/tcp  # DRBD
sudo ufw allow 2224/tcp  # pcsd
sudo ufw allow 3121/tcp  # pacemaker
sudo ufw allow 5405/tcp  # corosync
sudo ufw allow 21064/tcp # corosync
sudo ufw allow 9929/tcp  # corosync

# Para NFS
sudo ufw allow 2049/tcp  # NFS
sudo ufw allow 111/tcp   # portmapper
sudo ufw allow 20048/tcp # mountd

# Permitir tr√°fico entre nodos del cl√∫ster
sudo ufw allow from 192.168.10.0/24
sudo ufw allow from 10.0.0.0/8
```

## Optimizaciones para entorno virtualizado

### 1. Verificaci√≥n de configuraci√≥n de discos

```bash
# Verificar que los discos est√©n disponibles (en nodos 1 y 2)
sudo lsblk

# Deber√≠a mostrar algo como:
# sda     8:0    0   24G  0 disk 
# ‚îú‚îÄsda1  8:1    0   23G  0 part /
# ‚îî‚îÄsda2  8:2    0    1G  0 part [SWAP]
# sdb     8:16   0   16G  0 disk      <- Este es para DRBD

# En Node3 deber√≠a mostrar:
# sda     8:0    0   32G  0 disk 
# ‚îú‚îÄsda1  8:1    0   31G  0 part /
# ‚îî‚îÄsda2  8:2    0    1G  0 part [SWAP]
```

### 2. Optimizaciones de rendimiento para VMs

```bash
# Optimizaci√≥n de I/O para discos virtuales (en todas las VMs)
echo 'mq-deadline' | sudo tee /sys/block/sda/queue/scheduler

# Solo en nodos DRBD (1 y 2)
if [ -b /dev/sdb ]; then
    echo 'mq-deadline' | sudo tee /sys/block/sdb/queue/scheduler
fi

# Hacer persistente la configuraci√≥n
echo 'ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/scheduler}="mq-deadline"' | sudo tee /etc/udev/rules.d/60-scheduler.rules
```

### 3. Optimizaciones de red para DRBD (solo nodos 1 y 2)

```bash
# Ajustar par√°metros del kernel para mejor rendimiento
cat << EOF | sudo tee -a /etc/sysctl.conf
# Optimizaciones para DRBD
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144  
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# Optimizaciones adicionales para cl√∫ster
net.ipv4.tcp_congestion_control = bbr
net.core.netdev_max_backlog = 5000
EOF

# Aplicar cambios
sudo sysctl -p
```

## Scripts de automatizaci√≥n

### Script de instalaci√≥n para nodos DRBD (nodos 1 y 2)

```bash
#!/bin/bash
# install-drbd-node.sh
# Ejecutar como root en nodos 1 y 2

set -e

echo "=== Instalaci√≥n de paquetes DRBD ==="
# Actualizar sistema
apt update && apt upgrade -y

# Instalar paquetes necesarios
apt install -y \
    drbd-utils \
    drbd-dkms \
    pacemaker \
    corosync \
    pcs \
    crmsh \
    nfs-kernel-server \
    nfs-common \
    qemu-guest-agent \
    linux-headers-$(uname -r)

echo "=== Configuraci√≥n de servicios ==="
# Habilitar servicios
systemctl enable drbd
systemctl enable pacemaker  
systemctl enable corosync
systemctl enable pcsd
systemctl enable qemu-guest-agent

# Iniciar QEMU guest agent
systemctl start qemu-guest-agent

echo "=== Configuraci√≥n de usuario hacluster ==="
# Configurar usuario hacluster (cambiar la contrase√±a seg√∫n necesidades)
echo "hacluster:clusterpwd" | chpasswd

echo "=== Aplicaci√≥n de optimizaciones ==="
# Aplicar optimizaciones de red
cat >> /etc/sysctl.conf << EOF
# Optimizaciones para DRBD
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_congestion_control = bbr
EOF

sysctl -p

echo "=== Instalaci√≥n completada ==="
echo "IMPORTANTE: Reiniciar el sistema antes de continuar con la configuraci√≥n del cl√∫ster."
echo "Comando: sudo reboot"
```

### Script de instalaci√≥n para Node3 (Docker)

```bash
#!/bin/bash
# install-docker-node.sh
# Ejecutar como root en node3

set -e

echo "=== Instalaci√≥n de Docker ==="
# Actualizar sistema
apt update && apt upgrade -y

# Instalar utilidades b√°sicas
apt install -y \
    wget \
    curl \
    vim \
    nano \
    net-tools \
    htop \
    tree \
    qemu-guest-agent

echo "=== Instalaci√≥n de Docker ==="
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Configurar Docker
systemctl enable docker
systemctl start docker
systemctl enable qemu-guest-agent
systemctl start qemu-guest-agent

echo "=== Configuraci√≥n de usuario Docker ==="
# Agregar usuario al grupo docker (cambiar 'debian' por tu usuario)
USER_NAME=$(logname 2>/dev/null || echo "debian")
if id "$USER_NAME" &>/dev/null; then
    usermod -aG docker $USER_NAME
    echo "Usuario $USER_NAME agregado al grupo docker"
else
    echo "Usuario $USER_NAME no encontrado. Agregar manualmente con: usermod -aG docker [usuario]"
fi

echo "=== Instalaci√≥n completada ==="
echo "Docker versi√≥n: $(docker --version)"
echo "IMPORTANTE: Cerrar sesi√≥n y volver a iniciar para aplicar permisos de Docker."
```

### Script para verificar estado del cl√∫ster

```bash
#!/bin/bash
# check-cluster-status.sh

echo "=== Estado DRBD ==="
drbdadm status

echo -e "\n=== Estado Pacemaker ==="
pcs status

echo -e "\n=== Exportaciones NFS ==="
showmount -e localhost 2>/dev/null || echo "NFS no disponible"

echo -e "\n=== Recursos de red ==="
ip addr show | grep -A2 -E "(eth|enp)"
```

## Soluci√≥n de problemas espec√≠ficos

### 1. Problemas comunes con DRBD

```bash
# Si DRBD no inicia
systemctl status drbd
journalctl -u drbd

# Verificar configuraci√≥n
drbdadm dump all

# Problema con m√≥dulo del kernel
modprobe drbd
echo drbd >> /etc/modules
```

### 2. Problemas con Pacemaker

```bash
# Si pcs no funciona
systemctl status pcsd
systemctl restart pcsd

# Verificar comunicaci√≥n entre nodos
pcs status nodes
```

### 3. Problemas con NFS

```bash
# Verificar servicios NFS
systemctl status nfs-kernel-server
systemctl status rpcbind

# Verificar exportaciones
exportfs -v
```

## Consideraciones importantes

1. **Snapshots**: Realiza snapshots de las VMs en Proxmox antes de configurar el cl√∫ster
2. **Backup**: El cl√∫ster DRBD no reemplaza los backups regulares
3. **Monitoreo**: Considera herramientas como nagios o zabbix para supervisi√≥n
4. **Actualizaciones**: Ten cuidado con actualizaciones del kernel que requieren recompilar DRBD
5. **Red**: Mant√©n latencia baja (<1ms idealmente) entre nodos DRBD

---

**Autor**: Rodrigo √Ålvarez (@incogniadev)  
**Fecha**: 2025-07-21
