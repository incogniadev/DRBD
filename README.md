# Almacenamiento de alta disponibilidad DRBD para Docker

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![DRBD Version](https://img.shields.io/badge/DRBD-9.x-green.svg)](https://linbit.com/drbd/)
[![Pacemaker](https://img.shields.io/badge/Pacemaker-2.x-orange.svg)](https://clusterlabs.org/)
[![Docker](https://img.shields.io/badge/Docker-20.x+-blue.svg)](https://docker.com/)
[![NFS](https://img.shields.io/badge/NFS-v4-lightblue.svg)](https://en.wikipedia.org/wiki/Network_File_System)
[![Linux](https://img.shields.io/badge/OS-Linux-red.svg)](https://www.linux.org/)
[![High Availability](https://img.shields.io/badge/HA-Cluster-brightgreen.svg)](#)
[![Lab Environment](https://img.shields.io/badge/Environment-Laboratory-purple.svg)](#)
[![Architecture](https://img.shields.io/badge/Type-Architecture%20Design-blue.svg)](#)

## Descripci√≥n

Dise√±o de arquitectura y laboratorio de pruebas para implementar una soluci√≥n de alta disponibilidad para almacenamiento de contenedores Docker utilizando DRBD (Distributed Replicated Block Device) con gesti√≥n de cl√∫ster Pacemaker y servicios NFS. Este repositorio contiene las instrucciones detalladas y configuraciones necesarias para crear un entorno de laboratorio que demuestre esta arquitectura de alta disponibilidad.

## Caracter√≠sticas principales

- ‚úÖ **Alta disponibilidad** - Failover autom√°tico con tiempo de inactividad m√≠nimo
- ‚úÖ **Consistencia de datos** - Replicaci√≥n s√≠ncrona garantiza integridad
- ‚úÖ **Failover transparente** - Las aplicaciones contin√∫an ejecut√°ndose durante el failover
- ‚úÖ **Almacenamiento centralizado** - Punto √∫nico de gesti√≥n de almacenamiento para contenedores
- ‚úÖ **Escalabilidad** - F√°cil adici√≥n de nuevos hosts Docker como clientes NFS

## Documentaci√≥n

### üìã Gu√≠as disponibles

| Documento | Descripci√≥n |
|-----------|-------------|
| [üìê **Arquitectura del sistema**](docs/ARCHITECTURE.md) | Dise√±o completo y componentes de la arquitectura DRBD |
| [‚öôÔ∏è **Gu√≠a de instalaci√≥n**](docs/INSTALLATION.md) | Instrucciones generales de instalaci√≥n y configuraci√≥n |
| [üèóÔ∏è **Implementaci√≥n en Proxmox**](docs/PROXMOX_DEBIAN.md) | Gu√≠a espec√≠fica para entornos Proxmox con Debian |
| [üìù **Changelog**](CHANGELOG.md) | Historial de cambios del proyecto |

## Componentes del sistema

### Descripci√≥n general de nodos

| Nodo | Funci√≥n | IP Principal | IP del Cl√∫ster | Rol |
|------|---------|--------------|----------------|-----|
| **Node 1** | DRBD Primario | `10.0.0.231/8` | `192.168.10.231/24` | Almacenamiento activo, NFS activo |
| **Node 2** | DRBD Secundario | `10.0.0.232/8` | `192.168.10.232/24` | Replica en standby, NFS standby |
| **Node 3** | Host Docker | `10.0.0.233/8` | `192.168.10.233/24` | Ejecuci√≥n de contenedores |
| **VIP** | IP Flotante | - | `192.168.10.230/24` | Punto de acceso para alta disponibilidad |

### Caracter√≠sticas principales por nodo

#### üîµ Node 1 & Node 2: Nodos de almacenamiento DRBD
- Replicaci√≥n sincr√≥nica de datos en tiempo real
- Gesti√≥n autom√°tica de failover con Pacemaker
- Servicio NFS para compartir almacenamiento
- Dispositivos: `/dev/sdb1` ‚Üí `/dev/drbd0` ‚Üí `/mnt/docker-vol`

#### üü¢ Node 3: Host de ejecuci√≥n Docker
- **Almacenamiento 100% centralizado en NFS**
- Sin datos persistentes locales
- Acceso transparente v√≠a IP flotante
- Configuraci√≥n dual de red para administraci√≥n y cl√∫ster

## Requisitos del sistema

### üíª Hardware m√≠nimo recomendado

| Componente | Node 1 & 2 (DRBD) | Node 3 (Docker) |
|------------|-------------------|------------------|
| **CPU** | 2 vCPUs | 2 vCPUs |
| **RAM** | 2GB (4GB recomendado) | 4GB m√≠nimo |
| **Almacenamiento** | 20GB SO + 10GB DRBD | 30GB |
| **Red** | 2 interfaces (gesti√≥n + cl√∫ster) | 2 interfaces |

### üõ†Ô∏è Software requerido

| Componente | Versi√≥n | Notas |
|------------|---------|-------|
| **Linux OS** | Debian 11+, Ubuntu 20.04+, RHEL/CentOS 8+ | - |
| **DRBD** | 9.x+ | Con m√≥dulos del kernel |
| **Pacemaker** | 2.x+ | Gesti√≥n de cl√∫ster |
| **Corosync** | Compatible con Pacemaker | Comunicaci√≥n del cl√∫ster |
| **NFS** | v4+ | Cliente y servidor |
| **Docker** | 20.x+ | En Node 3 √∫nicamente |

## üöÄ Inicio r√°pido

Para comenzar con la implementaci√≥n del cl√∫ster DRBD de alta disponibilidad, sigue estos pasos:

### 1. Revisa la arquitectura
```bash
# Lee primero la documentaci√≥n de arquitectura
cat docs/ARCHITECTURE.md
```

### 2. Selecciona tu gu√≠a de instalaci√≥n

#### Instalaci√≥n general (cualquier Linux)
```bash
# Sigue la gu√≠a general de instalaci√≥n
cat docs/INSTALLATION.md
```

#### Instalaci√≥n espec√≠fica para Proxmox + Debian
```bash
# Para entornos virtualizados con Proxmox
cat docs/PROXMOX_DEBIAN.md
```

### 3. Verificaci√≥n post-instalaci√≥n

```bash
# Verificar estado del cl√∫ster DRBD
drbdadm status docker-vol

# Verificar estado de Pacemaker
pcs status

# Verificar montajes NFS
showmount -e 192.168.10.230

# Verificar Docker
docker info
```

## ‚ö° Proceso de failover autom√°tico

La arquitectura implementa un failover completamente autom√°tico:

1. üîç **Detecci√≥n de falla** ‚Üí Pacemaker detecta falla del nodo primario
2. üîÑ **Promoci√≥n de recursos** ‚Üí DRBD secundario se promueve a primario  
3. üìÅ **Montaje de filesystem** ‚Üí Sistema de archivos montado en nuevo nodo
4. üåê **Migraci√≥n de IP flotante** ‚Üí IP virtual migra al nodo activo
5. üîå **Reconexi√≥n autom√°tica** ‚Üí Docker se reconecta transparentemente

**Tiempo de failover t√≠pico: 30-60 segundos**

## üîß Comandos √∫tiles

### Monitoreo del cl√∫ster
```bash
# Estado general del cl√∫ster
pcs status

# Estado espec√≠fico de DRBD
drbdadm status docker-vol

# Verificar servicios NFS
showmount -e 192.168.10.230
```

### Mantenimiento
```bash
# Modo mantenimiento (standby)
pcs cluster standby node1

# Salir de modo mantenimiento
pcs cluster unstandby node1

# Backup de configuraci√≥n
pcs config backup cluster-backup.tar.bz2
```

Para m√°s detalles sobre monitoreo, mantenimiento y resoluci√≥n de problemas, consulta la [üìñ **gu√≠a de instalaci√≥n**](docs/INSTALLATION.md).

## Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'A√±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## Licencia

Este proyecto est√° licenciado bajo MIT License. Ver el archivo [LICENSE](LICENSE) para detalles.

## Autor

Dise√±o de arquitectura por Rodrigo Ernesto √Ålvarez Aguilera (@incogniadev) - Ingeniero DevOps en Promad Business Solutions

---

*Esta arquitectura proporciona una base robusta para cargas de trabajo containerizadas que requieren almacenamiento persistente y altamente disponible.*
